{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 4 - Question 4: Homography"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('hallway1.jpg')\n",
    "img2 = cv2.imread('hallway2.jpg')\n",
    "img3 = cv2.imread('hallway3.jpg')\n",
    "\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_RGB2BGR)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2BGR)\n",
    "img3 = cv2.cvtColor(img3, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Plot Selected Corresponding Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondence(case):\n",
    "    if case == \"A\":\n",
    "        pts1 = np.array([[999, 76],[1062, 13],[1095,228],[790, 614],[838,660]])\n",
    "        pts2 = np.array([[845, 391],[900, 331],[946, 537],[666, 937],[717, 981]])\n",
    "    elif case == \"B\":\n",
    "        pts1 = np.array([[999, 75],[1061, 12],[948,392],[788, 614],[1162,793]])\n",
    "        pts2 = np.array([[902, 266],[936,204],[882,582],[795,807],[1022,986]])\n",
    "    elif case == \"C\":\n",
    "        pts1 = np.array([[653,544],[789,614],[840,660],[483,777],[536,645]])\n",
    "        pts2 = np.array([[687,740],[795,807],[826,854],[415,982],[528,845]])\n",
    "    else:\n",
    "        return \n",
    "    return pts1, pts2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correspondence(case, image1, image2):\n",
    "    pts1, pts2 = get_correspondence(case)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 10))\n",
    "    fig.suptitle(f'Case {case} Correspondences', y=0.7, fontsize=12)\n",
    "\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    ax1.imshow(gray1, cmap='gray')\n",
    "    ax2.imshow(gray2, cmap='gray')\n",
    "\n",
    "    for i in range(5):\n",
    "        ax1.plot(pts1[i, 0], pts1[i, 1], \"rs\", fillstyle=\"none\")\n",
    "        ax2.plot(pts2[i, 0], pts2[i, 1], \"rs\", fillstyle=\"none\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correspondence(\"A\", img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correspondence(\"B\", img1, img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correspondence(\"C\", img1, img3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Fit a Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_homography(pts1, pts2):\n",
    "    n = len(pts1)\n",
    "    A = np.zeros((2*n, 9))\n",
    "    for i in range(5):\n",
    "        x1, y1 = pts1[i, 0], pts1[i, 1]\n",
    "        x2, y2 = pts2[i, 0], pts2[i, 1]\n",
    "        A[2*i] = [-x1, -y1, -1, 0, 0, 0, x1*x2, y1*x2, x2]\n",
    "        A[2*i+1] = [0, 0, 0, -x1, -y1, -1, x1*y2, y1*y2, y2]\n",
    "\n",
    "    # Solve for the homography matrix\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    L = Vt[-1] / Vt[-1, -1]\n",
    "    H = L.reshape(3, 3)\n",
    "\n",
    "    return H\n",
    "\n",
    "def find_case_homo(case):\n",
    "    pts1, pts2 = get_correspondence(case)\n",
    "    return fit_homography(pts1, pts2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_case_homo(\"A\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "- Translation: the translation vector is approximately (-238.57, 311.97), indicating that the transformed image has been shifted to the left and up relative to the original image.\n",
    "- Scale: the scaling factors are 1.18 and 1.055 respectively for x and y directions, meaning the image is slightly up-scaled.\n",
    "- Shear: very small figures that can be ignored\n",
    "- Rotation: very small figures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_case_homo(\"B\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "- Translation: the translation vector is approximately (311.95, 192.81), indicating that the transformed image has been shifted to right and up.\n",
    "- Scale: the scaling factors are 0.598 and 0.991 respectively for x and y directions, meaning the image is down-sampled.\n",
    "- Shear: shearing factors (0.0123, 0.0013) means the figure is slightly sheared in both directions.\n",
    "- Rotation: very small figures that can be ignored "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_case_homo(\"C\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "- Translation: the translation vector is approximately (333.14, 285.35), indicating that the transformed image has been shifted to the right and up relative to the original image.\n",
    "- Scale: the image has been downsampled by 0.787 in each direction.  \n",
    "- Rotation: transformed image has been rotated counterclockwise\n",
    "- Shear: -0.4585 and -0.1072 represent shear to the left and down"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use Homography to Map Selected Point to Estimated Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapped_coordinate(H, x, y):\n",
    "\n",
    "    v = np.array([x, y, 1.])\n",
    "    v = H @ v\n",
    "    z = v[2]\n",
    "    v = v / z\n",
    "    est_x, est_y = round(v[0]), round(v[1])\n",
    "\n",
    "    return est_x, est_y\n",
    "\n",
    "def estimate_coordinates(pts, H):\n",
    "    \n",
    "    estimated = []\n",
    "\n",
    "    for pt in pts:\n",
    "        x, y = pt\n",
    "        est_x, est_y = get_mapped_coordinate(H, x, y)\n",
    "        estimated.append([est_x, est_y])\n",
    "\n",
    "    return np.array(estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_est_homo(case, image):\n",
    "    img_copy = image.copy()\n",
    "\n",
    "    pts1, pts2 = get_correspondence(case)\n",
    "    H = fit_homography(pts1, pts2)\n",
    "    pts_est = estimate_coordinates(pts1, H)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.imshow(img_copy)\n",
    "\n",
    "    for i in range(5):\n",
    "        ax.plot(pts_est[i, 0], pts_est[i, 1], \"gs\", fillstyle=\"none\", markersize=10)\n",
    "    for i in range(5):  \n",
    "        ax.plot(pts2[i, 0], pts2[i, 1], \"rs\", fillstyle=\"none\", markersize=13)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_est_homo_case(case):\n",
    "    if case == \"A\":\n",
    "        plot_est_homo(\"A\", img2)\n",
    "    elif case == \"B\":\n",
    "        plot_est_homo(\"B\", img3)\n",
    "    elif case == \"C\":\n",
    "        plot_est_homo(\"C\", img3)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est_homo_case(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est_homo_case(\"B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_est_homo_case(\"C\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Inverse Mapped Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_mapped_pixel(case, image1, image2, plot=True):\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    pts1, pts2 = get_correspondence(case)\n",
    "    H = fit_homography(pts1, pts2)\n",
    "    pts_est = estimate_coordinates(pts1, H)\n",
    "\n",
    "    # initialize to zero\n",
    "    padding = 500\n",
    "    x, y, z = gray1.shape[0] + padding * 2, gray1.shape[1] + padding * 2,  3\n",
    "\n",
    "    # fill red channel with values of the original image\n",
    "    new_img = np.zeros((x, y, z))\n",
    "    new_img[padding:x-padding, padding:y-padding, 0] = gray1\n",
    "\n",
    "    # blue and green channels\n",
    "\n",
    "    for x in range(new_img.shape[0]):\n",
    "        for y in range(new_img.shape[1]):\n",
    "            y_, x_ = get_mapped_coordinate(H, y, x) # note x, y is reversed\n",
    "            if x_ >= padding and x_ < padding + gray2.shape[0] \\\n",
    "                and y_ >= padding and y_ < padding + gray2.shape[1]:\n",
    "                new_img[x, y, 1] = gray2[x_ - padding, y_ - padding]\n",
    "                new_img[x, y, 2] = gray2[x_ - padding, y_ - padding]\n",
    "    new_img /= 255\n",
    "    if plot:\n",
    "        plt.imshow(new_img)\n",
    "    else:\n",
    "        return new_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_mapped_pixel(\"A\", img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_mapped_pixel(\"B\", img1, img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_mapped_pixel(\"C\", img1, img3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Homography contains perspective transformations consist of scale, translation, shear, and rotation\n",
    "- Camera position is about the same in image1 and image2, with camera orientation up in image2.\n",
    "- Camera positions are different in image1 and image3, also camera orientation is slightly counter-clockwisely rotated.\n",
    "- The floor is of less Lambertian reflectance while the right wall has more Lambertian reflectance. As we can observe from the three images, the floor has a \"mirror-like\" reflections while the right wall is more of matte texture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Putting together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = input(\"What's the case you want to run? Type 'A', 'B', or 'C' without quotation marks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(case, image1, image2):\n",
    "    fig, axes = plt.subplots(2, 2,figsize=(15,15))\n",
    "    fig.suptitle(f'Case {case}',y=0.85, fontsize=16)\n",
    "\n",
    "    pts1, pts2 = get_correspondence(case)\n",
    "\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    axes[0,0].imshow(image1)\n",
    "    axes[0,0].set_title('4.1: 1st Image Selected Points')\n",
    "    axes[0,1].imshow(image2)\n",
    "    axes[0,1].set_title('4.1: 2nd Image Selected Points')\n",
    "\n",
    "    for i in range(5):\n",
    "        axes[0,0].plot(pts1[i, 0], pts1[i, 1], \"rs\", fillstyle=\"none\")\n",
    "        axes[0,1].plot(pts2[i, 0], pts2[i, 1], \"rs\", fillstyle=\"none\")\n",
    "\n",
    "    H = fit_homography(pts1, pts2)\n",
    "\n",
    "    pts_est = estimate_coordinates(pts1, H)\n",
    "\n",
    "    axes[1,0].imshow(image2)\n",
    "    axes[1,0].set_title('4.3: Estimated Points Using Homography')\n",
    "    for i in range(5):\n",
    "        axes[1,0].plot(pts_est[i, 0], pts_est[i, 1], \"gs\", fillstyle=\"none\", markersize=10)\n",
    "    for i in range(5):  \n",
    "        axes[1,0].plot(pts2[i, 0], pts2[i, 1], \"rs\", fillstyle=\"none\", markersize=13)\n",
    "    \n",
    "    new_img = inverse_mapped_pixel(case, image1, image2, False)\n",
    "    axes[1,1].imshow(new_img)\n",
    "    axes[1,1].set_title('4.4: Inverse Mapped Pixels')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if case == \"A\":    \n",
    "    run(\"A\", img1, img2)\n",
    "elif case == \"B\":\n",
    "    run(\"B\", img1, img3)\n",
    "elif case == \"C\":\n",
    "     run(\"C\", img1, img3)\n",
    "else:\n",
    "     print(\"Invalid case number\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
